{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpovnqrZCTnM",
        "outputId": "9a9547a6-f8eb-4ee2-9d51-94b3f13087c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping at epoch 319\n",
            "\n",
            "==================== TRAINING SUMMARY ====================\n",
            "Total Epochs Used: 319\n",
            "Final Loss: 0.174733\n",
            "\n",
            "Input Data (X):\n",
            " [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "\n",
            "Target Output (y):\n",
            " [[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n",
            "\n",
            "Initial Weights Layer 1:\n",
            " [[ 0.49671415 -0.1382643   0.64768854  1.52302986]\n",
            " [-0.23415337 -0.23413696  1.57921282  0.76743473]]\n",
            "Initial Bias Layer 1:\n",
            " [[-0.46947439  0.54256004 -0.46341769 -0.46572975]]\n",
            "\n",
            "Initial Weights Layer 2:\n",
            " [[ 0.24196227]\n",
            " [-1.91328024]\n",
            " [-1.72491783]\n",
            " [-0.56228753]]\n",
            "Initial Bias Layer 2:\n",
            " [[-1.01283112]]\n",
            "\n",
            "-----------------------------------------------\n",
            "Final Weights Layer 1:\n",
            " [[ 21.58247024 -20.56320003 -20.43346894  24.0117975 ]\n",
            " [ 17.57390747 -21.63234466  21.56489899  21.09123143]]\n",
            "Final Bias Layer 1:\n",
            " [[-1.13156993  0.18133697  2.94857857 -1.99120494]]\n",
            "\n",
            "Final Weights Layer 2:\n",
            " [[  0.64213963]\n",
            " [-14.71749449]\n",
            " [ -1.81830192]\n",
            " [  0.9841282 ]]\n",
            "Final Bias Layer 2:\n",
            " [[0.42782053]]\n",
            "-----------------------------------------------\n",
            "\n",
            "--- XOR Gate Results (After Training with Adam + Regularization) ---\n",
            " Input 1  Input 2  Target Output  Predicted Output\n",
            "       0        0              0               0.0\n",
            "       0        1              1               1.0\n",
            "       1        0              1               1.0\n",
            "       1        1              0               1.0\n",
            "===============================================================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Activation Functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# XOR Data\n",
        "X = np.array([[0,0],\n",
        "              [0,1],\n",
        "              [1,0],\n",
        "              [1,1]])\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "# Hyperparameters\n",
        "np.random.seed(42)\n",
        "input_size, hidden_size, output_size = 2, 4, 1\n",
        "learning_rate = 0.05\n",
        "epochs = 10000\n",
        "l2_lambda = 0.001\n",
        "dropout_rate = 0.2\n",
        "beta1, beta2 = 0.9, 0.999\n",
        "epsilon = 1e-8\n",
        "patience = 300\n",
        "\n",
        "# Parameter Initialization\n",
        "weights1 = np.random.randn(input_size, hidden_size)\n",
        "bias1 = np.random.randn(1, hidden_size)\n",
        "weights2 = np.random.randn(hidden_size, output_size)\n",
        "bias2 = np.random.randn(1, output_size)\n",
        "\n",
        "# Store initial parameters\n",
        "initial_weights1 = weights1.copy()\n",
        "initial_weights2 = weights2.copy()\n",
        "initial_bias1 = bias1.copy()\n",
        "initial_bias2 = bias2.copy()\n",
        "\n",
        "# Adam optimizer variables\n",
        "m_w1, v_w1 = np.zeros_like(weights1), np.zeros_like(weights1)\n",
        "m_b1, v_b1 = np.zeros_like(bias1), np.zeros_like(bias1)\n",
        "m_w2, v_w2 = np.zeros_like(weights2), np.zeros_like(weights2)\n",
        "m_b2, v_b2 = np.zeros_like(bias2), np.zeros_like(bias2)\n",
        "\n",
        "# Early stopping setup\n",
        "best_loss = np.inf\n",
        "no_improve = 0\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # Forward pass\n",
        "    z1 = np.dot(X, weights1) + bias1\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    # Dropout\n",
        "    dropout_mask = (np.random.rand(*a1.shape) > dropout_rate).astype(float)\n",
        "    a1 *= dropout_mask\n",
        "    a1 /= (1 - dropout_rate)\n",
        "\n",
        "    z2 = np.dot(a1, weights2) + bias2\n",
        "    output = sigmoid(z2)\n",
        "\n",
        "    # Compute loss (MSE + L2 Regularization)\n",
        "    loss = np.mean((y - output) ** 2) + l2_lambda * (np.sum(weights1 ** 2) + np.sum(weights2 ** 2))\n",
        "\n",
        "    # Backpropagation\n",
        "    d_output = (y - output) * sigmoid_derivative(output)\n",
        "    d_weights2 = np.dot(a1.T, d_output) / len(X) + l2_lambda * weights2\n",
        "    d_bias2 = np.mean(d_output, axis=0, keepdims=True)\n",
        "\n",
        "    d_hidden = np.dot(d_output, weights2.T) * sigmoid_derivative(a1)\n",
        "    d_weights1 = np.dot(X.T, d_hidden) / len(X) + l2_lambda * weights1\n",
        "    d_bias1 = np.mean(d_hidden, axis=0, keepdims=True)\n",
        "\n",
        "    # Adam Optimizer Update\n",
        "    m_w1 = beta1 * m_w1 + (1 - beta1) * d_weights1\n",
        "    v_w1 = beta2 * v_w1 + (1 - beta2) * (d_weights1 ** 2)\n",
        "    m_b1 = beta1 * m_b1 + (1 - beta1) * d_bias1\n",
        "    v_b1 = beta2 * v_b1 + (1 - beta2) * (d_bias1 ** 2)\n",
        "\n",
        "    m_w2 = beta1 * m_w2 + (1 - beta1) * d_weights2\n",
        "    v_w2 = beta2 * v_w2 + (1 - beta2) * (d_weights2 ** 2)\n",
        "    m_b2 = beta1 * m_b2 + (1 - beta1) * d_bias2\n",
        "    v_b2 = beta2 * v_b2 + (1 - beta2) * (d_bias2 ** 2)\n",
        "\n",
        "    # Bias correction\n",
        "    m_w1_hat = m_w1 / (1 - beta1 ** epoch)\n",
        "    v_w1_hat = v_w1 / (1 - beta2 ** epoch)\n",
        "    m_b1_hat = m_b1 / (1 - beta1 ** epoch)\n",
        "    v_b1_hat = v_b1 / (1 - beta2 ** epoch)\n",
        "\n",
        "    m_w2_hat = m_w2 / (1 - beta1 ** epoch)\n",
        "    v_w2_hat = v_w2 / (1 - beta2 ** epoch)\n",
        "    m_b2_hat = m_b2 / (1 - beta1 ** epoch)\n",
        "    v_b2_hat = v_b2 / (1 - beta2 ** epoch)\n",
        "\n",
        "    # Update parameters\n",
        "    weights1 += learning_rate * m_w1_hat / (np.sqrt(v_w1_hat) + epsilon)\n",
        "    bias1 += learning_rate * m_b1_hat / (np.sqrt(v_b1_hat) + epsilon)\n",
        "    weights2 += learning_rate * m_w2_hat / (np.sqrt(v_w2_hat) + epsilon)\n",
        "    bias2 += learning_rate * m_b2_hat / (np.sqrt(v_b2_hat) + epsilon)\n",
        "\n",
        "    # Early stopping\n",
        "    if loss < best_loss - 1e-6:\n",
        "        best_loss = loss\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "    if no_improve > patience:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "# Final Evaluation\n",
        "final_hidden = sigmoid(np.dot(X, weights1) + bias1)\n",
        "final_output = sigmoid(np.dot(final_hidden, weights2) + bias2)\n",
        "predicted = np.round(final_output)\n",
        "\n",
        "# Display Results\n",
        "print(\"\\n==================== TRAINING SUMMARY ====================\")\n",
        "print(f\"Total Epochs Used: {epoch}\")\n",
        "print(f\"Final Loss: {best_loss:.6f}\\n\")\n",
        "\n",
        "print(\"Input Data (X):\\n\", X)\n",
        "print(\"\\nTarget Output (y):\\n\", y)\n",
        "\n",
        "print(\"\\nInitial Weights Layer 1:\\n\", initial_weights1)\n",
        "print(\"Initial Bias Layer 1:\\n\", initial_bias1)\n",
        "print(\"\\nInitial Weights Layer 2:\\n\", initial_weights2)\n",
        "print(\"Initial Bias Layer 2:\\n\", initial_bias2)\n",
        "\n",
        "print(\"\\n-----------------------------------------------\")\n",
        "print(\"Final Weights Layer 1:\\n\", weights1)\n",
        "print(\"Final Bias Layer 1:\\n\", bias1)\n",
        "print(\"\\nFinal Weights Layer 2:\\n\", weights2)\n",
        "print(\"Final Bias Layer 2:\\n\", bias2)\n",
        "print(\"-----------------------------------------------\")\n",
        "\n",
        "# Tabular Output\n",
        "results = pd.DataFrame({\n",
        "    \"Input 1\": X[:, 0],\n",
        "    \"Input 2\": X[:, 1],\n",
        "    \"Target Output\": y.flatten(),\n",
        "    \"Predicted Output\": predicted.flatten()\n",
        "})\n",
        "\n",
        "print(results.to_string(index=False))\n",
        "print(\"===============================================================\")\n"
      ]
    }
  ]
}